{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evKPSi0pswQP"
   },
   "source": [
    "# using tensorflow version 1.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hl5dTGwvsSb",
    "outputId": "b3c4d983-a815-44d0-8f6d-c0ddc4d17e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeANaVobs38a"
   },
   "source": [
    "# directly downloading the twitter corpus from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01yH3BjR1Nof",
    "outputId": "365f7465-a4ee-4847-82eb-ca42c4a7bfa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.3M  100 20.3M    0     0  62.1M      0 --:--:-- --:--:-- --:--:-- 62.1M\n"
     ]
    }
   ],
   "source": [
    "# source : https://github.com/Marsan-Ma-zz/chat_corpus\n",
    "!curl --header 'Host: raw.githubusercontent.com' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https://github.com/marsan-ma/chat_corpus/blob/master/twitter_en.txt.gz' --header 'Upgrade-Insecure-Requests: 1' 'https://raw.githubusercontent.com/marsan-ma/chat_corpus/master/twitter_en.txt.gz' --output 'twitter_en.txt.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywv3HjGjtb7m"
   },
   "source": [
    "# directly downloading the cornell_movie_dialogs_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nzq5SlFvyiF",
    "outputId": "54d4fd88-d58d-4511-91fd-5c477ec5a345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9684k  100 9684k    0     0  36.9M      0 --:--:-- --:--:-- --:--:-- 36.9M\n"
     ]
    }
   ],
   "source": [
    "!curl --header 'Host: www.cs.cornell.edu' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --header 'Upgrade-Insecure-Requests: 1' 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip' --output 'cornell_movie_dialogs_corpus.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6E3yjhWt3N6"
   },
   "source": [
    "# extracting archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "Un9kIUmTwUfk",
    "outputId": "ffaed4cf-34e7-4522-dba3-561602b198ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting patool\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "\r",
      "\u001b[K     |████▎                           | 10kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 20kB 32.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 30kB 19.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 40kB 23.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 51kB 22.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 61kB 25.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 71kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 81kB 8.2MB/s \n",
      "\u001b[?25hInstalling collected packages: patool\n",
      "Successfully installed patool-1.12\n",
      "patool: Extracting /content/twitter_en.txt.gz ...\n",
      "patool: running /usr/bin/7z e -o./Unpack_egq7qwcg -- /content/twitter_en.txt.gz\n",
      "patool: ... /content/twitter_en.txt.gz extracted to `chat.txt'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'chat.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install patool\n",
    "import patoolib\n",
    "patoolib.extract_archive('/content/twitter_en.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "bnt8QCjuApEi",
    "outputId": "7e0ad509-7818-448e-b18b-c4ca1e168782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting /content/cornell_movie_dialogs_corpus.zip ...\n",
      "patool: running /usr/bin/7z x -o./Unpack_hc76zpjn -- /content/cornell_movie_dialogs_corpus.zip\n",
      "patool: ... /content/cornell_movie_dialogs_corpus.zip extracted to `cornell_movie_dialogs_corpus' (multiple files in root).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cornell_movie_dialogs_corpus'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive('/content/cornell_movie_dialogs_corpus.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6jM8rLLt88t"
   },
   "source": [
    "#cloning github repo for getting friends corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6XPWVX78zvl",
    "outputId": "e98a6c39-c235-4700-ed1c-858afedae152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Friends_Analysis'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 499 (delta 8), reused 13 (delta 4), pack-reused 472\u001b[K\n",
      "Receiving objects: 100% (499/499), 61.32 MiB | 25.18 MiB/s, done.\n",
      "Resolving deltas: 100% (171/171), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/shilpibhattacharyya/Friends_Analysis.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M66OeKX9uIh9"
   },
   "source": [
    "# importing google drive for storing checkpoints and other important stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgeMpVWwQwZH",
    "outputId": "88a852c6-c69f-4efb-b56e-0ee3a8b222a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcnZUOs9uQfs"
   },
   "source": [
    "installing demoji library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxAZVtoa_fqq",
    "outputId": "ca9bf39d-ff8b-4d25-cdf4-0c3dbf4c5215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
      "Installing collected packages: colorama, demoji\n",
      "Successfully installed colorama-0.4.4 demoji-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wkI2T65uVWw"
   },
   "source": [
    "# use this code for making data ready for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKnq-IMDwWGv",
    "outputId": "54883a90-dafe-4aad-95e1-37fe174cefa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m (Got response in 0.13 seconds)\n",
      "\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m\n",
      "Preparing raw data into train set and test set ...\n",
      "chat.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"data.py\", line 361, in <module>\n",
      "    prepare_raw_data()\n",
      "  File \"data.py\", line 258, in prepare_raw_data\n",
      "    questions, answers = get_lines()\n",
      "  File \"data.py\", line 34, in get_lines\n",
      "    line = normalizeString(line)\n",
      "  File \"data.py\", line 81, in normalizeString\n",
      "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
      "  File \"/usr/lib/python3.6/re.py\", line 191, in sub\n",
      "KeyboardInterrupt\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "oBXYN8zDPNdq",
    "outputId": "a4457ac0-7d02-442d-9b72-0a4afca34663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting /content/drive/MyDrive/processed.zip ...\n",
      "patool: running /usr/bin/7z x -o./Unpack_pads0i1u -- /content/drive/MyDrive/processed.zip\n",
      "patool: ... /content/drive/MyDrive/processed.zip extracted to `processed1' (local file exists).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'processed1'"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive('/content/drive/MyDrive/processed.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "V1DxTZldn5YQ"
   },
   "outputs": [],
   "source": [
    "#!rm -r /content/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_D2LQhiKqdh"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/drive/MyDrive/processed.zip  /content/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XThV4DmSugPA"
   },
   "source": [
    "# use this code for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wS74ublLw0z_",
    "outputId": "9e2742dd-f181-4a84-a3c7-841f2aa8e648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m (Got response in 0.13 seconds)\n",
      "\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m\n",
      "Data ready!\n",
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 29999\n",
      "Bucketing conversation number 39999\n",
      "Bucketing conversation number 49999\n",
      "Bucketing conversation number 59999\n",
      "Bucketing conversation number 69999\n",
      "Bucketing conversation number 79999\n",
      "Bucketing conversation number 89999\n",
      "Bucketing conversation number 99999\n",
      "Bucketing conversation number 109999\n",
      "Bucketing conversation number 119999\n",
      "Bucketing conversation number 129999\n",
      "Bucketing conversation number 139999\n",
      "Bucketing conversation number 149999\n",
      "Bucketing conversation number 159999\n",
      "Bucketing conversation number 169999\n",
      "Bucketing conversation number 179999\n",
      "Bucketing conversation number 189999\n",
      "Bucketing conversation number 199999\n",
      "Bucketing conversation number 209999\n",
      "Bucketing conversation number 219999\n",
      "Bucketing conversation number 229999\n",
      "Bucketing conversation number 239999\n",
      "Bucketing conversation number 249999\n",
      "Bucketing conversation number 259999\n",
      "Bucketing conversation number 269999\n",
      "Bucketing conversation number 279999\n",
      "Bucketing conversation number 289999\n",
      "Bucketing conversation number 299999\n",
      "Bucketing conversation number 309999\n",
      "Bucketing conversation number 319999\n",
      "Bucketing conversation number 329999\n",
      "Bucketing conversation number 339999\n",
      "Bucketing conversation number 349999\n",
      "Bucketing conversation number 359999\n",
      "Bucketing conversation number 369999\n",
      "Bucketing conversation number 379999\n",
      "Bucketing conversation number 389999\n",
      "Bucketing conversation number 399999\n",
      "Bucketing conversation number 409999\n",
      "Bucketing conversation number 419999\n",
      "Bucketing conversation number 429999\n",
      "Bucketing conversation number 439999\n",
      "Bucketing conversation number 449999\n",
      "Bucketing conversation number 459999\n",
      "Bucketing conversation number 469999\n",
      "Bucketing conversation number 479999\n",
      "Bucketing conversation number 489999\n",
      "Bucketing conversation number 499999\n",
      "Bucketing conversation number 509999\n",
      "Bucketing conversation number 519999\n",
      "Bucketing conversation number 529999\n",
      "Bucketing conversation number 539999\n",
      "Bucketing conversation number 549999\n",
      "Bucketing conversation number 559999\n",
      "Bucketing conversation number 569999\n",
      "Bucketing conversation number 579999\n",
      "Bucketing conversation number 589999\n",
      "Number of samples in each bucket:\n",
      " [290892, 282554]\n",
      "Bucket scale:\n",
      " [0.5072700829720671, 1.0]\n",
      "Initialize new model\n",
      "Create placeholders\n",
      "WARNING:tensorflow:From /content/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Create inference\n",
      "WARNING:tensorflow:From /content/model.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:50: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /content/model.py:51: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Creating loss... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:859: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Time: 9.633638858795166\n",
      "Create optimizer... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /content/model.py:95: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:99: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Creating opt for bucket 0 took 8.127853393554688 seconds\n",
      "Creating opt for bucket 1 took 17.10442066192627 seconds\n",
      "WARNING:tensorflow:From chatbot.py:132: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:134: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Running session\n",
      "WARNING:tensorflow:From chatbot.py:136: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading parameters for the Chatbot\n",
      "Iter 15000: loss 2.8773401846885682, time 0.7796683311462402\n",
      "Test bucket 0: loss 3.6931562423706055, time 2.737347364425659\n",
      "Test bucket 1: loss 4.007665157318115, time 6.733011722564697\n",
      "Iter 15500: loss 2.87376633310318, time 1.6867163181304932\n",
      "Iter 16000: loss 2.8431011776924136, time 0.741368293762207\n",
      "Iter 16500: loss 2.8399336400032045, time 1.754838228225708\n",
      "Iter 17000: loss 2.8259790081977845, time 1.7991058826446533\n",
      "Iter 17500: loss 2.8398639812469484, time 0.8019511699676514\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Iter 18000: loss 2.816554583072662, time 0.7987918853759766\n",
      "Iter 18500: loss 2.7730008463859557, time 1.7754120826721191\n",
      "Iter 19000: loss 2.7965106620788576, time 0.7856214046478271\n"
     ]
    }
   ],
   "source": [
    "!python chatbot.py --mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F98NJMmk4jX"
   },
   "source": [
    "# for trainig model on joey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40g70mbHk35g",
    "outputId": "8c5e56b7-cd50-47af-a8fb-772df9bf1a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m (Got response in 0.14 seconds)\n",
      "\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m\n",
      "Data ready!\n",
      "reading friends corpus /content/Friends_Analysis/transcripts_friends/season_all/merged.csv\n",
      "len of dialouges 16249\n",
      "Initialize new model\n",
      "Create placeholders\n",
      "WARNING:tensorflow:From /content/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Create inference\n",
      "WARNING:tensorflow:From /content/model.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:50: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /content/model.py:51: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Creating loss... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:859: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Time: 9.218035697937012\n",
      "Create optimizer... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /content/model.py:95: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:99: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Creating opt for bucket 0 took 7.514506578445435 seconds\n",
      "Creating opt for bucket 1 took 16.764139652252197 seconds\n",
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 29999\n",
      "Bucketing conversation number 39999\n",
      "Bucketing conversation number 49999\n",
      "Bucketing conversation number 59999\n",
      "Bucketing conversation number 69999\n",
      "Bucketing conversation number 79999\n",
      "Bucketing conversation number 89999\n",
      "Bucketing conversation number 99999\n",
      "Bucketing conversation number 109999\n",
      "Bucketing conversation number 119999\n",
      "Bucketing conversation number 129999\n",
      "Bucketing conversation number 139999\n",
      "Bucketing conversation number 149999\n",
      "Bucketing conversation number 159999\n",
      "Bucketing conversation number 169999\n",
      "Bucketing conversation number 179999\n",
      "Bucketing conversation number 189999\n",
      "Bucketing conversation number 199999\n",
      "Bucketing conversation number 209999\n",
      "Bucketing conversation number 219999\n",
      "Bucketing conversation number 229999\n",
      "Bucketing conversation number 239999\n",
      "Bucketing conversation number 249999\n",
      "Bucketing conversation number 259999\n",
      "Bucketing conversation number 269999\n",
      "Bucketing conversation number 279999\n",
      "Bucketing conversation number 289999\n",
      "Bucketing conversation number 299999\n",
      "Bucketing conversation number 309999\n",
      "Bucketing conversation number 319999\n",
      "Bucketing conversation number 329999\n",
      "Bucketing conversation number 339999\n",
      "Bucketing conversation number 349999\n",
      "Bucketing conversation number 359999\n",
      "Bucketing conversation number 369999\n",
      "Bucketing conversation number 379999\n",
      "Bucketing conversation number 389999\n",
      "Bucketing conversation number 399999\n",
      "Bucketing conversation number 409999\n",
      "Bucketing conversation number 419999\n",
      "Bucketing conversation number 429999\n",
      "Bucketing conversation number 439999\n",
      "Bucketing conversation number 449999\n",
      "Bucketing conversation number 459999\n",
      "Bucketing conversation number 469999\n",
      "Bucketing conversation number 479999\n",
      "Bucketing conversation number 489999\n",
      "Bucketing conversation number 499999\n",
      "Bucketing conversation number 509999\n",
      "Bucketing conversation number 519999\n",
      "Bucketing conversation number 529999\n",
      "Bucketing conversation number 539999\n",
      "Bucketing conversation number 549999\n",
      "Bucketing conversation number 559999\n",
      "Bucketing conversation number 569999\n",
      "Bucketing conversation number 579999\n",
      "Bucketing conversation number 589999\n",
      "Number of samples in each bucket:\n",
      " [290892, 282554]\n",
      "Bucket scale:\n",
      " [0.5072700829720671, 1.0]\n",
      "WARNING:tensorflow:From chatbot.py:339: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:341: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:342: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading parameters for the Chatbot\n",
      "16249\n",
      "Iter 22050: loss 0.06613868705928326, time 1.6873397827148438\n",
      "Iter 22100: loss 0.12940621107816697, time 0.7211074829101562\n",
      "Iter 22150: loss 0.19747367173433303, time 1.68693208694458\n",
      "Iter 22200: loss 0.25922729693353175, time 1.7108228206634521\n",
      "Iter 22200: loss 0.06480682423338294, time 1.710876703262329\n",
      "Iter 22250: loss 0.06157390832901001, time 0.7176244258880615\n",
      "Iter 22300: loss 0.1252700562775135, time 0.722052812576294\n",
      "Iter 22350: loss 0.1880980657786131, time 1.7231311798095703\n",
      "Iter 22400: loss 0.24810063250362874, time 0.7174160480499268\n",
      "Iter 22400: loss 0.062025158125907184, time 0.7174737453460693\n",
      "Iter 22450: loss 0.061792055144906044, time 1.7060787677764893\n",
      "Iter 22500: loss 0.12543159283697605, time 0.7135179042816162\n",
      "Iter 22550: loss 0.18637660384178162, time 0.7140777111053467\n",
      "Iter 22600: loss 0.2450263250619173, time 1.7141423225402832\n",
      "Iter 22600: loss 0.061256581265479323, time 1.71421217918396\n",
      "Iter 22650: loss 0.06056884057819843, time 1.7131261825561523\n",
      "Iter 22700: loss 0.11911242939531803, time 1.695291519165039\n",
      "Iter 22750: loss 0.1787260153889656, time 0.7187089920043945\n",
      "Iter 22800: loss 0.2382506860792637, time 1.7135112285614014\n",
      "Iter 22800: loss 0.059562671519815924, time 1.7136366367340088\n",
      "Iter 22850: loss 0.060729004293680194, time 0.7165384292602539\n",
      "Iter 22900: loss 0.12090703837573528, time 1.6904866695404053\n",
      "Iter 22950: loss 0.17801697932183744, time 1.7212822437286377\n",
      "Iter 23000: loss 0.24128009557723998, time 1.7212469577789307\n",
      "Iter 23000: loss 0.060320023894309995, time 1.721294641494751\n",
      "Iter 23050: loss 0.0638284581899643, time 1.7119789123535156\n",
      "Iter 23100: loss 0.12435934625566006, time 1.7815675735473633\n",
      "Iter 23150: loss 0.18413790479302405, time 1.6955769062042236\n",
      "Iter 23200: loss 0.24378098368644716, time 1.7017900943756104\n",
      "Iter 23200: loss 0.06094524592161179, time 1.701866626739502\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Iter 23250: loss 0.06083355471491814, time 1.7520687580108643\n",
      "Iter 23300: loss 0.123587099686265, time 0.7104935646057129\n",
      "Iter 23350: loss 0.18580830715596675, time 0.7138016223907471\n",
      "Iter 23400: loss 0.25031899601221086, time 0.7217447757720947\n",
      "Iter 23400: loss 0.06257974900305271, time 0.7218155860900879\n",
      "Iter 23450: loss 0.06523749835789204, time 0.7047817707061768\n",
      "Iter 23500: loss 0.14142015248537063, time 1.7228200435638428\n",
      "Iter 23550: loss 0.2659048828482628, time 1.6961712837219238\n",
      "Iter 23600: loss 0.6381719636917115, time 0.7089855670928955\n",
      "Iter 23600: loss 0.15954299092292787, time 0.7090320587158203\n",
      "Iter 23650: loss 0.47773839473724367, time 0.7088944911956787\n",
      "Iter 23700: loss 0.7499749526381493, time 1.715787410736084\n",
      "Iter 23750: loss 0.8982507859170437, time 1.7268831729888916\n",
      "Iter 23800: loss 1.0006786172091962, time 1.6771855354309082\n",
      "Iter 23800: loss 0.25016965430229904, time 1.6773128509521484\n",
      "Iter 23850: loss 0.07461611926555634, time 0.7244598865509033\n",
      "Iter 23900: loss 0.14572164803743362, time 1.7004475593566895\n",
      "Iter 23950: loss 0.21203874662518502, time 0.7158262729644775\n",
      "Iter 24000: loss 0.27426321372389795, time 1.7248883247375488\n",
      "Iter 24000: loss 0.06856580343097449, time 1.7249469757080078\n",
      "Test bucket 0: loss 0.06946377456188202, time 2.371098518371582\n",
      "Test bucket 1: loss 0.06451675295829773, time 6.329207897186279\n",
      "Iter 24050: loss 0.06262784026563167, time 1.7269396781921387\n",
      "Iter 24100: loss 0.12463845483958721, time 0.7139041423797607\n",
      "Iter 24150: loss 0.1825206933170557, time 1.7165577411651611\n",
      "Iter 24200: loss 0.2432566986232996, time 0.7518572807312012\n",
      "Iter 24200: loss 0.0608141746558249, time 0.7519147396087646\n",
      "Iter 24250: loss 0.059281836971640585, time 1.7121906280517578\n",
      "Iter 24300: loss 0.11612396374344826, time 1.7251427173614502\n",
      "Iter 24350: loss 0.17331585928797721, time 0.7218716144561768\n",
      "Iter 24400: loss 0.23232243433594704, time 1.696160078048706\n",
      "Iter 24400: loss 0.05808060858398676, time 1.6962320804595947\n",
      "Iter 24450: loss 0.057189037203788755, time 1.7311172485351562\n",
      "Iter 24500: loss 0.11133791856467724, time 1.7542948722839355\n",
      "Iter 24550: loss 0.16735481671988964, time 1.71437668800354\n",
      "Iter 24600: loss 0.22597558982670307, time 0.697016716003418\n",
      "Iter 24600: loss 0.05649389745667577, time 0.697075366973877\n",
      "Iter 24650: loss 0.058422740772366526, time 0.7097926139831543\n",
      "Iter 24700: loss 0.1156136491894722, time 0.7091619968414307\n",
      "Iter 24750: loss 0.17123451210558416, time 0.7191531658172607\n",
      "Iter 24800: loss 0.2286414062231779, time 0.7159214019775391\n",
      "Iter 24800: loss 0.05716035155579448, time 0.7160341739654541\n",
      "Iter 24850: loss 0.05692652404308319, time 0.7215163707733154\n",
      "Iter 24900: loss 0.11781391210854053, time 1.696833610534668\n",
      "Iter 24950: loss 0.17200008198618888, time 1.6945834159851074\n",
      "Iter 25000: loss 0.2290073649585247, time 0.7070333957672119\n",
      "Iter 25000: loss 0.057251841239631174, time 0.7070832252502441\n",
      "Iter 25050: loss 0.05873774081468582, time 1.672149658203125\n",
      "Iter 25100: loss 0.11589750602841377, time 1.6930408477783203\n",
      "Iter 25150: loss 0.17063673183321953, time 0.7070732116699219\n",
      "Iter 25200: loss 0.22976157911121844, time 1.6951146125793457\n",
      "Iter 25200: loss 0.05744039477780461, time 1.6951632499694824\n",
      "Traceback (most recent call last):\n",
      "  File \"chatbot.py\", line 432, in <module>\n",
      "  File \"chatbot.py\", line 429, in main\n",
      "    main()\n",
      "  File \"chatbot.py\", line 364, in train_joey\n",
      "    _, step_loss, _ = run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, False)\n",
      "  File \"chatbot.py\", line 74, in run_step\n",
      "    outputs = sess.run(output_feed, input_feed)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "KeyboardInterrupt\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python chatbot.py --mode train_joey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M97xPGBvumMb"
   },
   "source": [
    "# run this code snippet for chating with the bot after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7roZ50KdxJ65",
    "outputId": "943757a3-670e-4493-f14f-a3a11cea4fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m (Got response in 0.14 seconds)\n",
      "\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[32m... OK\u001b[0m\n",
      "Data ready!\n",
      "Initialize new model\n",
      "Create placeholders\n",
      "WARNING:tensorflow:From /content/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Create inference\n",
      "WARNING:tensorflow:From /content/model.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/model.py:50: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /content/model.py:51: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Creating loss... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:859: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Time: 9.730226278305054\n",
      "Create optimizer... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /content/model.py:95: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:207: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:211: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From chatbot.py:212: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading parameters for the Chatbot\n",
      "Welcome to TensorBro. Say something. Enter to exit and type \"feedback:\" followed by correction, Max length is 40\n",
      "> hey.\n",
      "how s it going ?\n",
      "> good, how are you?\n",
      "i i m you\n",
      "> no you are not me.\n",
      "so then\n",
      "> you are joey.\n",
      "you want to go ?\n",
      "> where will you take me?\n",
      "what ?\n",
      "> where would you like to go?\n",
      "right just go in and then give your your start on your face\n",
      "> no i dont want that.\n",
      "here s up on your date okay ?\n",
      "> noo.\n",
      "uh oh you got # # #\n",
      "> what are you doing?\n",
      "you and rachel found a great work you know ? the ring is before i could do it !\n",
      "> ya we founf out the ring.\n",
      "you got to go see when to say goodbye and that you will and mike you and i can\n",
      "> do you like me?\n",
      "about little hours joey\n",
      "> i guess no.\n",
      "come on you should just come in you and me first together and you both cut like a real\n",
      "> no i dont want to get anywhere with you.\n",
      "okay what are you like ?\n",
      "> i like you.\n",
      "well you re not getting a little of your mom this is a lot of stuff i could do\n",
      "> what cann you do.\n",
      "i don t like that either ! all right look i can t tell you that she s the\n",
      "> what ?\n",
      "the free room down down the door the duck ! you re gonna have to get out of his\n",
      "> okay getting out of this now.\n",
      "i got off this can i ride all the murders and i have to go to the bathroom\n",
      "> okay see you.\n",
      "how do you mean ?\n",
      "> i mean bye.\n",
      "now bye ?\n",
      "> yes bye. good night.\n",
      "you re you re you ?\n",
      "> bye.\n",
      "go home and do something for me not to wear a women s room i can go to the\n",
      "> okay . love you.\n",
      "okay ?\n",
      "> yes.\n",
      "alright ? is it like in your mind ?\n",
      "> yes its in my mind.\n",
      "you wanna come on me and watch em ! and me ?\n",
      "> yes.\n",
      "alright ? is it like in your mind ?\n",
      "> yes, its in my mind.\n",
      "you wanna come on me and watch em ! and me ?\n",
      "> no.\n",
      "rach it s not a little maybe she s ever ever ever ever ever since when your dad friends\n",
      "> okay.\n",
      "how is she ?\n",
      "> goodbye.\n",
      "think it s going to be okay ? i was kinda cold for something !\n",
      "> bye.\n",
      "go home and do something for me not to wear a women s room i can go to the\n",
      "> \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python chatbot.py --mode chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJCUGotritWX",
    "outputId": "afaa973b-cbfe-4308-9a50-9efb16ef959b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/drive/MyDrive/content/checkpoints/ (stored 0%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8000.data-00000-of-00001 (deflated 9%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8000.index (deflated 51%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8000.meta (deflated 96%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8500.data-00000-of-00001 (deflated 9%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8500.index (deflated 51%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-8500.meta (deflated 96%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9000.index (deflated 51%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9000.data-00000-of-00001 (deflated 9%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9000.meta (deflated 96%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9500.index (deflated 51%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9500.data-00000-of-00001 (deflated 9%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-9500.meta (deflated 96%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-10000.data-00000-of-00001 (deflated 9%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-10000.index (deflated 51%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/checkpoint (deflated 80%)\n",
      "  adding: content/drive/MyDrive/content/checkpoints/chatbot-10000.meta (deflated 96%)\n"
     ]
    }
   ],
   "source": [
    "#!zip -r /content/drive/MyDrive/processed.zip  /content/drive/MyDrive/content/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kL67_y7nmGZ4",
    "outputId": "678f4137-d5da-4ce1-9772-63013e26811e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/processed/ (stored 0%)\n",
      "  adding: content/processed/test.enc (deflated 60%)\n",
      "  adding: content/processed/vocab.enc (deflated 51%)\n",
      "  adding: content/processed/train.enc (deflated 62%)\n",
      "  adding: content/processed/train_ids.enc (deflated 61%)\n",
      "  adding: content/processed/test_ids.enc (deflated 59%)\n",
      "  adding: content/processed/test_ids.dec (deflated 62%)\n",
      "  adding: content/processed/train.dec (deflated 61%)\n",
      "  adding: content/processed/train_ids.dec (deflated 63%)\n",
      "  adding: content/processed/test.dec (deflated 60%)\n",
      "  adding: content/processed/vocab.dec (deflated 51%)\n"
     ]
    }
   ],
   "source": [
    "#!zip -r /content/drive/MyDrive/processed.zip  /content/processed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
